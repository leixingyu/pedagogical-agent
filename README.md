## Introduction

__Project:__ Multimodal Affective Pedagogical Agents for Different Types of Learners

__Faculty:__ Nicoletta Adamo, Purdue (PI), Richard E. Mayer, UCSB (co-PI), Bedrich Benes, Purdue (co-PI)

__Sponsor:__ NSF - IIS - Cyberlearning, award # 1821894 (2018-2021), Purdue Instructional Innovation Grant (2018- 2020)

http://hpcg.purdue.edu/idealab/AffectiveAgents/


## Project Overview

__Goal__: To create a fully procedural animated character

__Demo__: https://youtu.be/E59mXjoXK5g

__Features__:

1. State-machine based pose interpolation system

2. IK arm and leg, and other high level joint controls

3. Automated facial expression and features like lip-sync, eye gaze, eye blink

4. Speech anaylsis and beat detection

5. BML controlled procedural animation 

__Components and Parameters__:

1. Body Gesture

- [pose index] Destination body pose
- [speed] Animation Speed
- [blend] Duration of animtion transition

2. Facial Expression

- [emotion] Range from four types of emotion (angry, sad, happy, bored)
- [strength] Strength of Emotion

3. Hand Pose

- [side] Left or Right Side of the hand
- [shape] Shape of the hand (relaxed, holding fist, pointing)

4. Foot Status

- [status] Lock to the ground / Float / Procedural stepping

__Illustration__:


## Plugin

[Final IK] https://assetstore.unity.com/packages/tools/animation/final-ik-14290

[Salsa Suite] https://assetstore.unity.com/packages/tools/animation/salsa-lipsync-suite-148442

[Mecanim Control] https://assetstore.unity.com/packages/tools/animation/mecanim-control-15156

